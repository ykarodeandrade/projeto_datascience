{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 1 - Ciência dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nome: Ykaro de Sousa Andrade"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tema do projeto \n",
    "## Produto: Marca multinacional PUMA.\n",
    "\n",
    "Tal escolha foi feita pois desde que o jogador de futebol Neymar Jr assumiu como o mais novo garoto propaganda da Puma, a marca começou a ter um maior engajamento nas redes sociais. Por este motivo, analisar os dados obtidos após tal repercussão no twitter é fator primordial para que haja um melhor entendimento da opinião dos clientes e assim aprimorar as próximas campanhas de marketing da empresa por exemplo.\n",
    "\n",
    "\n",
    "Foi obtido uma base de dados com tweets postados pelos usuários da rede social. A partir disso, classificou-se os tweets entre relevantes e irrelevantes. A partir disso **classificou-se como tweets relevantes todos aqueles que enalteciam a marca**, e em contra partida **classificou-se como tweets irrelevante todos aqueles que falavam mal da marca ou não tinham relação direta com a Puma**.\n",
    "\n",
    "\n",
    "Dessarte, ao analisar os tweets, percebeu-se que os assuntos mais frequentemente relacionados a marca se tratavam de:\n",
    "\n",
    "•\tDesde que o Neymar foi contratado para ser garoto propaganda, a Puma começou a fabricar produtos mais bonitos;\n",
    "\n",
    "•\t Devido a alguma peça do vestuário da Puma que o Neymar foi visto usando, o público tinha maior interesse em adiquirir o produto;\n",
    "\n",
    "•\t Observou-se muitos tweets relacionados à marca onde a enquadrava como pertencente ao top3 marcas mundiais, tais como: Nike e Adidas;\n",
    "\n",
    "•   Internautas comentaram sobre o design das peças estarem mais bonitos;\n",
    "\n",
    "•\tTweets relacionados a satisfação dos fãs desde que que Neymar se tornou patrocinado da Puma;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Carregando algumas bibliotecas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importando bibliotecas\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import display\n",
    "pd.options.display.max_rows = 13\n",
    "import emoji\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esperamos trabalhar no diretório\n",
      "C:\\Users\\ykaro\\OneDrive - Insper - Institudo de Ensino e Pesquisa\\Documentos\\3 semestre\\Ciência dos Dados\\Projeto Classificador Naive Bayes\\projeto_datascience\n"
     ]
    }
   ],
   "source": [
    "print('Esperamos trabalhar no diretório')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definindo função de limpeza dos caracteres dos tweets e espaçando os emajis e palavras corretamente.\n",
    "\n",
    "def cleanup(text):\n",
    "    #import string\n",
    "    punctuation = '[!-.:?;\\n)(*$#@''\"\"]' # Note que os sinais [] são delimitadores de um conjunto.\n",
    "    pattern = re.compile(punctuation)\n",
    "    text_subbed = re.sub(pattern, '', text)\n",
    "    text_subbed = emoji.get_emoji_regexp().split(text_subbed) #emoji\n",
    "    return ' '.join(text_subbed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando a base de dados com os tweets classificados como relevantes e não relevantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'puma.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treinamento</th>\n",
       "      <th>Irrelevante 0 / Relevante 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>o neymar inovou a puma https://t.co/qtcuvd1svj</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a @puma tá deitando nas chuteiras do neymar, m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fico assim no da puma affss https://t.co/1ygyh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@g_estrella__ puma tá aí , mas três listra tá ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@0ketlyn_s eu acho que um puma ^^</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Treinamento  \\\n",
       "0     o neymar inovou a puma https://t.co/qtcuvd1svj   \n",
       "1  a @puma tá deitando nas chuteiras do neymar, m...   \n",
       "2  fico assim no da puma affss https://t.co/1ygyh...   \n",
       "3  @g_estrella__ puma tá aí , mas três listra tá ...   \n",
       "4                  @0ketlyn_s eu acho que um puma ^^   \n",
       "\n",
       "   Irrelevante 0 / Relevante 1  \n",
       "0                            1  \n",
       "1                            1  \n",
       "2                            0  \n",
       "3                            1  \n",
       "4                            0  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_excel(filename)\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teste</th>\n",
       "      <th>Irrelevante 0 / Relevante 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@0chavex0 @ornate_puma vdd to resfriado</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>📽 neymar e puma 👀❤!!\\ntemos novidades vindo aí...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@brgmsch mas a puma nem patrocina a aston</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>📽 puma neymar jr creativity  a inovadora chute...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a puma tá a evoluir muito, quem me dera que o ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Teste  \\\n",
       "0            @0chavex0 @ornate_puma vdd to resfriado   \n",
       "1  📽 neymar e puma 👀❤!!\\ntemos novidades vindo aí...   \n",
       "2          @brgmsch mas a puma nem patrocina a aston   \n",
       "3  📽 puma neymar jr creativity  a inovadora chute...   \n",
       "4  a puma tá a evoluir muito, quem me dera que o ...   \n",
       "\n",
       "   Irrelevante 0 / Relevante 1  \n",
       "0                            0  \n",
       "1                            1  \n",
       "2                            0  \n",
       "3                            1  \n",
       "4                            1  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_excel(filename, sheet_name = 'Teste')\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Classificador automático de sentimento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**O projeto consiste na análise de tweets da marca esportiva Puma.**\n",
    "\n",
    "\n",
    "Foi definido como premissa que os tweets relevantes seriam todos aqueles que exaltavam a marca em seus comentários, em contra partida os tweets irrelevantes seriam todos os outros, ou seja, os que não tinham relação direta com a marca e/ou falavam mal da Puma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando um Classificador Naive-Bayes\n",
    "\n",
    "Considerando apenas as mensagens da planilha Treinamento, ensine  seu classificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todos os tweets da base de dados :\n",
    "dados = pd.read_excel('puma.xlsx')\n",
    "#dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando apenas a planilha de Treinamento :\n",
    "t_treinamento = dados.loc[:, 'Treinamento']\n",
    "#t_treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processo de limpeza dos tweets de treinamento usando a função que definimos como cleanup.\n",
    "lista_limpa = []\n",
    "for index, argumento in enumerate (t_treinamento):\n",
    "    #print(f'posição {index} tweeter: {argumento}')\n",
    "    limpa = cleanup(argumento.lower())\n",
    "    lista_limpa.append(limpa)\n",
    "    \n",
    "#print(lista_limpa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Criando um novo data frame com os tweets de treinamentos limpos\n",
    "dados['Limpo'] = lista_limpa\n",
    "#dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrando apenas os tweets relevantes e os adicionando em uma nova variável\n",
    "relevante = dados.loc[dados['Irrelevante 0 / Relevante 1'] == 1, 'Limpo']\n",
    "#relevante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1° for: Separar os tweets relevantes em palavras relevantes dando um split em cada tweet.\n",
    "p_relevantes = [] \n",
    "for c,conteudo in enumerate (relevante):\n",
    "    p_relevantes.append(conteudo.split())\n",
    "    \n",
    "#print(p_relevantes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Definindo todas as palvras revelantes em um conjunto\n",
    "todas_palavras_relevantes=[]\n",
    "for index, conteudo in enumerate(p_relevantes):\n",
    "    cont=0\n",
    "    while cont < len(p_relevantes[index]):\n",
    "        novo= conteudo[cont]\n",
    "        todas_palavras_relevantes.append(novo)\n",
    "        cont+=1\n",
    "#print(todas_palavras_relevantes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " O total de Palavras relevantes é 1730\n"
     ]
    }
   ],
   "source": [
    "# Informando o total de palavras dos tweets relevantes\n",
    "print(f' O total de Palavras relevantes é {len(todas_palavras_relevantes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo uma nova limpeza, retirando todos os 'https', pois está str não é relevante para nossa análise\n",
    "lista_de_links = []\n",
    "for index, conteudo in enumerate(todas_palavras_relevantes):\n",
    "    #print(f'{index}: {conteudo}')\n",
    "    cont = 0\n",
    "    while cont < len(todas_palavras_relevantes[index]):\n",
    "        #print(conteudo[cont])\n",
    "        if conteudo[cont:5] == 'https':\n",
    "            #print(f' palvra: {lista[index]} na posição {index}')\n",
    "            lista_de_links.append(todas_palavras_relevantes[index])\n",
    "        cont+=1\n",
    "\n",
    "lista_nova_r = []\n",
    "cont2 = 0\n",
    "while cont2 < len(todas_palavras_relevantes):\n",
    "    if todas_palavras_relevantes[cont2] not in lista_de_links:\n",
    "        lista_nova_r.append(todas_palavras_relevantes[cont2])\n",
    "    cont2+=1\n",
    "\n",
    "# Atualização da variável todas_palavras_relevantes sem os https:\n",
    "\n",
    "todas_palavras_relevantes= lista_nova_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " O total de Palavras relevantes sem a string \"htpps\" é 1691\n"
     ]
    }
   ],
   "source": [
    "print(f' O total de Palavras relevantes sem a string \"htpps\" é {len(todas_palavras_relevantes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Guardando as palavras como um pd.Series\n",
    "serie_relevante = pd.Series(todas_palavras_relevantes)\n",
    "\n",
    "# Frequencia absoluta de palavras relevantes\n",
    "tabela_relevante = serie_relevante.value_counts()\n",
    "#tabela_relevante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"Fazendo o mesmo para as palavras irrelevantes\"></div>\n",
    "\n",
    "# Fazendo o mesmo para as palavras irrelevantes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrando apenas os tweets irrelevantes e os adicionando em uma nova variável\n",
    "irrelevante = dados.loc[dados['Irrelevante 0 / Relevante 1'] == 0, 'Limpo']\n",
    "#irrelevante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1° for: Separar os tweets irrelevantes em palavras irrelevantes dando um split em cada tweet.\n",
    "p_irrelevantes = []\n",
    "for index, conteudo in enumerate (irrelevante):\n",
    "    p_irrelevantes.append(conteudo.split())\n",
    "    \n",
    "# print(p_irrelevantes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Definindo todas as palvras irrevelantes em um conjunto\n",
    "todas_palavras_irrelevantes=[]\n",
    "for index, conteudo in enumerate(p_irrelevantes):\n",
    "    cont=0\n",
    "    while cont < len(p_irrelevantes[index]):\n",
    "        novo= conteudo[cont]\n",
    "        todas_palavras_irrelevantes.append(novo)\n",
    "        cont+=1\n",
    "#print(todas_palavras_irrelevantes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " O total de Palavras irrelevantes é 3115\n"
     ]
    }
   ],
   "source": [
    "# Informando o total de palavras dos tweets relevantes\n",
    "print(f' O total de Palavras irrelevantes é {len(todas_palavras_irrelevantes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Aprimorando a limpeza dos Tweets\n",
    "# Fazendo limpeza de links iniciados com http. \n",
    "\n",
    "lista_de_links= []\n",
    "for index, conteudo in enumerate(todas_palavras_irrelevantes):\n",
    "    #print(f'{index}: {conteudo}')\n",
    "    cont = 0\n",
    "    while cont < len(todas_palavras_irrelevantes[index]):\n",
    "        #print(conteudo[cont])\n",
    "        if conteudo[cont:5] == 'https':\n",
    "            #print(f' palvra: {lista[index]} na posição {index}')\n",
    "            lista_de_links.append(todas_palavras_irrelevantes[index])\n",
    "        cont+=1\n",
    "\n",
    "lista_nova_irr = []\n",
    "cont2 = 0\n",
    "while cont2 < len(todas_palavras_irrelevantes):\n",
    "    if todas_palavras_irrelevantes[cont2] not in lista_de_links:\n",
    "        lista_nova_irr.append(todas_palavras_irrelevantes[cont2])\n",
    "    cont2+=1\n",
    "\n",
    "# Atualização da variável todas_palavras_relevantes sem os https:\n",
    "\n",
    "todas_palavras_irrelevantes= lista_nova_irr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " O total de Palavras irrelevantes sem os \"htpps\": 3067\n"
     ]
    }
   ],
   "source": [
    "print(f' O total de Palavras irrelevantes sem os \"htpps\": {len(todas_palavras_irrelevantes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardando as palavras irrelevantes como um pd.Series\n",
    "serie_irrelevante = pd.Series(todas_palavras_irrelevantes)\n",
    "\n",
    "# Frequencia absoluta de palavras relevantes\n",
    "tabela_irrelevante = serie_irrelevante.value_counts()\n",
    "#tabela_irrelevante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todas as palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O total de palavras com repetição é 4758\n"
     ]
    }
   ],
   "source": [
    "# Total de palavras com repetição.\n",
    "total_de_palavras = todas_palavras_relevantes + todas_palavras_irrelevantes\n",
    "print(f'O total de palavras com repetição é {len(total_de_palavras)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = pd.concat([serie_relevante, serie_irrelevante])\n",
    "#x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_p_sem_r = x.value_counts().shape[0]\n",
    "#total_p_sem_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teorema de Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35540142917192097\n",
      "0.644598570828079\n",
      "4758\n"
     ]
    }
   ],
   "source": [
    "# Calculando as probabilidades \n",
    "\n",
    "Probabilidade_de_ser_relevante = len(todas_palavras_relevantes)/len(total_de_palavras)\n",
    "Probabilidade_de_ser_irrelevante = len(todas_palavras_irrelevantes)/len(total_de_palavras)\n",
    "print(Probabilidade_de_ser_relevante)\n",
    "print(Probabilidade_de_ser_irrelevante)\n",
    "print(len(total_de_palavras))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Verificando a performance do Classificador\n",
    "\n",
    "Agora você deve testar o seu classificador com a base de Testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.59\n",
       "1    0.41\n",
       "Name: Irrelevante 0 / Relevante 1, dtype: float64"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planilha_teste = pd.read_excel('puma.xlsx', sheet_name = 'Teste')\n",
    "planilha_teste['Irrelevante 0 / Relevante 1'].value_counts(True)\n",
    "#planilha_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando apenas a planilha de Testes.\n",
    "t_teste = planilha_teste.loc[:, 'Teste']\n",
    "#t_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpando os tweets de teste usando a função que definimos como cleanup.\n",
    "lista2_limpa = []\n",
    "for index, argumento in enumerate (t_teste):\n",
    "    #print(f'posição {index} tweeter: {argumento}')\n",
    "    limpa = cleanup(argumento.lower())\n",
    "    lista2_limpa.append(limpa)\n",
    "    \n",
    "#print(lista2_limpa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1° for: Separar os tweets relevantes em palavras relevantes dando um split\n",
    "lista_geral_teste= [] \n",
    "for c,conteudo in enumerate (lista2_limpa):\n",
    "    lista_geral_teste.append(conteudo.split())\n",
    "    \n",
    "#print(lista_geral_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teorema de Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"a-ingenuidade-naive-bayes\"></div>\n",
    "\n",
    "### A ingenuidade - Naïve Bayes\n",
    "\n",
    "Agora vamos a parte da ingênua do Naïve Bayes.\n",
    "Tal igenuidade consiste em assumir que as palavras são independentes entre si e que sua ordem na frase não importa. \n",
    "\n",
    "\n",
    "\n",
    "$\\quad P(tweet|R) = \n",
    "P( o |R)\\cdot P( neymar |R)\\cdot P( inovou |R)\\cdot P( a |R)\\cdot P( puma |R)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Vamos denotar **Tweets Relevantes** simplesmente como $R$, para ficar melhor a visualização para a aplicação do Teorema de Bayes.\n",
    "\n",
    "Assim, a fórmula completa fica:\n",
    "\n",
    "$P(R|tweet) = \\frac{P(o|R).P(neymar|R).P(inovou|R).P(a|R).P(puma|R).P(R)}{P(tweet)}$\n",
    "\n",
    "<br>\n",
    "\n",
    "Da mesma forma, denotando **Os Tweets irrelevantes** como $IRR$, a fórmula completa fica:\n",
    "\n",
    "$P(IRR|tweet) = \\frac{P(o|IRR).P(neymar|IRR).P(inovou|IRR).P(a|IRR).P(puma|IRR).P(IRR)}{P(tweet)}$\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "Note que o denominador das duas probabilidade condicionais acima é o mesmo, para fazer a classificação do tweet podemos cancelar o cálculo do denominador $P(tweet)$. \n",
    "\n",
    "Assim, a **Classificação da frase** se dará conforme abaixo:\n",
    "\n",
    "$\\quad \\Rightarrow$ Se $P(R|tweet) > P(IRR|tweet)$, então frase será classificada como um *Tweet Relevante*.\n",
    "\n",
    "$\\quad \\Rightarrow$ Se $P(R|tweet) < P(IRR|tweet)$, então frase será classificada como um *Tweet Irrelevante*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo classificação dos tweets da planilha teste\n",
    "\n",
    "lista_classificador = []\n",
    "\n",
    "laplace_relevante = Probabilidade_de_ser_relevante\n",
    "laplace_irrelevante = Probabilidade_de_ser_irrelevante\n",
    "\n",
    "\n",
    "for tweet in lista_geral_teste:\n",
    "    \n",
    "    for palavra in tweet:\n",
    "    \n",
    "        if palavra not in tabela_relevante and palavra not in tabela_irrelevante:\n",
    "            laplace_relevante *= (0 + 1) / (len(todas_palavras_relevantes) + total_p_sem_r)\n",
    "            laplace_irrelevante *= (0 + 1) / (len(todas_palavras_irrelevantes) + total_p_sem_r)\n",
    "\n",
    "\n",
    "        elif palavra not in tabela_irrelevante and palavra in tabela_relevante:\n",
    "            laplace_irrelevante *= (0 + 1) / (len(todas_palavras_irrelevantes) + total_p_sem_r)\n",
    "            laplace_relevante *= (tabela_relevante[palavra] + 1) / (len(todas_palavras_relevantes) + total_p_sem_r)\n",
    "\n",
    "        elif palavra not in tabela_relevante and palavra in tabela_irrelevante:\n",
    "            laplace_relevante *= (0 + 1) / (len(todas_palavras_relevantes) + total_p_sem_r)\n",
    "            laplace_irrelevante *= (tabela_irrelevante[palavra] + 1) / (len(todas_palavras_irrelevantes) + total_p_sem_r)\n",
    "\n",
    "\n",
    "\n",
    "        elif palavra in tabela_relevante and palavra in tabela_irrelevante:\n",
    "            laplace_relevante *= (tabela_relevante[palavra] + 1) / (len(todas_palavras_relevantes) + total_p_sem_r)\n",
    "            laplace_irrelevante *= (tabela_irrelevante[palavra] + 1) / (len(todas_palavras_irrelevantes) + total_p_sem_r)\n",
    "\n",
    "\n",
    "\n",
    "    if laplace_relevante > laplace_irrelevante:\n",
    "        lista_classificador.append(1)\n",
    "    else:\n",
    "        lista_classificador.append(0)\n",
    "    \n",
    "    laplace_relevante = Probabilidade_de_ser_relevante\n",
    "    laplace_irrelevante = Probabilidade_de_ser_irrelevante\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teste</th>\n",
       "      <th>Irrelevante 0 / Relevante 1</th>\n",
       "      <th>Algoritimo Classificador</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@0chavex0 @ornate_puma vdd to resfriado</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>📽 neymar e puma 👀❤!!\\ntemos novidades vindo aí...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@brgmsch mas a puma nem patrocina a aston</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>📽 puma neymar jr creativity  a inovadora chute...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a puma tá a evoluir muito, quem me dera que o ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>neymar de puma 🤤</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@anthrpocene puma é a única música deles que n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@slbdiogo_ warning: eu quero a puma este mano ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@srtonalves palmeiras tem dinheiro de vendas d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>agora fudeu o caralho todo mermo vao puma porra</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Teste  \\\n",
       "0            @0chavex0 @ornate_puma vdd to resfriado   \n",
       "1  📽 neymar e puma 👀❤!!\\ntemos novidades vindo aí...   \n",
       "2          @brgmsch mas a puma nem patrocina a aston   \n",
       "3  📽 puma neymar jr creativity  a inovadora chute...   \n",
       "4  a puma tá a evoluir muito, quem me dera que o ...   \n",
       "5                                   neymar de puma 🤤   \n",
       "6  @anthrpocene puma é a única música deles que n...   \n",
       "7  @slbdiogo_ warning: eu quero a puma este mano ...   \n",
       "8  @srtonalves palmeiras tem dinheiro de vendas d...   \n",
       "9    agora fudeu o caralho todo mermo vao puma porra   \n",
       "\n",
       "   Irrelevante 0 / Relevante 1  Algoritimo Classificador  \n",
       "0                            0                         0  \n",
       "1                            1                         1  \n",
       "2                            0                         0  \n",
       "3                            1                         1  \n",
       "4                            1                         0  \n",
       "5                            1                         1  \n",
       "6                            0                         0  \n",
       "7                            0                         1  \n",
       "8                            0                         0  \n",
       "9                            0                         0  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planilha_teste[\"Algoritimo Classificador\"] = lista_classificador\n",
    "planilha_teste.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Algoritimo Classificador</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Irrelevante 0 / Relevante 1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.330</td>\n",
       "      <td>0.260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.085</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Algoritimo Classificador         0      1\n",
       "Irrelevante 0 / Relevante 1              \n",
       "0                            0.330  0.260\n",
       "1                            0.085  0.325"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = pd.crosstab(planilha_teste['Irrelevante 0 / Relevante 1'], planilha_teste['Algoritimo Classificador'], normalize=True)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performace do Classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A performace do Classificador de tweets é de 65.5%\n"
     ]
    }
   ],
   "source": [
    "y = m[0][0] + m[1][1]\n",
    "print(f'A performace do Classificador de tweets é de {100*y}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A acurácia do Classificador é de 65.5%. Podemos observar que a classificação de falsos 0's se mostrou satisfatória. \n",
    "Por outro lado a classificação de falsos 1's ficou bem alta, isso ocorreu por conta de tweets que possuiam ironias,\n",
    "gírias e dupla negação, o que é característico no nicho de futebol e público jovem. O classificador não consegue identificar o sentimentos atribuidos às palavras apenas analisando as probabilidade Bayesianas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porquê não pode usar o próprio classificador para gerar mais amostras de treinamento?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não podemos utilizar o próprio classificador para alimentar nossa base de treinamento, pois o mesmo possuiria um viés na classificação do novos tweets, visto que as frequências de palavras relevantes e irrelevantes estariam baseadas no Data Frame de treinamento inicial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diferentes cenários em que Naïve Bayes pode ser aplicado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pode-se utilizar o classificador Naive Bayes em interfaces de Email, por exemplo Gmail e Outlook, a classificação de quais emails estarão nas abas de destaque, outros ou spam são classificadas a partir de Naive Bayes. Com isso os emails mais relevantes são adionados nos destaques e os prováveis menos relevantes são adicionados à caixa de spam.\n",
    "\n",
    "Outro exemplo de aplicação para Naive Bayes são as tags de vídeos do YouTube.\n",
    "Quando um produtor de conteúdo (YouTuber) posta seus vídeos, ele adiciona tags relacionadas ao conteúdo referente ao vídeo. Dessa forma, quando o usuário vai pesquisar por uma palavra relacionada à essa tag, o vídeo é recomendado como mais provável para o usuário."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sugestões de melhorias para a performace do Classificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma possível iteração para o código é ter uma base de dados maior para Treinamento, além de ter mais pessoas para realizarem a\n",
    "classificação desses tweets. Assim, a base de dados conterá mais palavras e o viés de escolha será diminuído, pois\n",
    "haverá mais pessoas participando da classificação (diversidade de opinião).\n",
    "\n",
    "Outra iteração para o código é criar uma função que identifique palavras que foram escritas juntas e as separem.\n",
    "Dessa forma, as duas ou mais palavras irão compor as tabela de frequencias de palavras relevantes e de palavras irrelevantes de maneira correta e assim a acurácida do nosso Classificador será mais precisa.\n",
    "\n",
    "Material de apoio: https://towardsdatascience.com/?gi=e75d71ded956"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Qualidade do Classificador a partir de novas separações dos tweets entre Treinamento e Teste\n",
    "\n",
    "Caso for fazer esse item do Projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Aperfeiçoamento:\n",
    "\n",
    "Os trabalhos vão evoluir em conceito dependendo da quantidade de itens avançados:\n",
    "\n",
    "* Limpar: \\n, :, \", ', (, ), etc SEM remover emojis\n",
    "* Corrigir separação de espaços entre palavras e emojis ou entre emojis e emojis\n",
    "* Propor outras limpezas e transformações que não afetem a qualidade da informação ou classificação\n",
    "* Criar categorias intermediárias de relevância baseadas na probabilidade: ex.: muito relevante, relevante, neutro, irrelevante, muito irrelevante (3 categorias: C, mais categorias conta para B)\n",
    "* Explicar por que não posso usar o próprio classificador para gerar mais amostras de treinamento\n",
    "* Propor diferentes cenários para Naïve Bayes fora do contexto do projeto\n",
    "* Sugerir e explicar melhorias reais com indicações concretas de como implementar (indicar como fazer e indicar material de pesquisa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Referências Bibliográficas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)\n",
    "Acesso realizado no dia 10/09/2021\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/)\n",
    "Acesso realizado no dia 10/09/2021\n",
    "\n",
    "https://towardsdatascience.com/?gi=e75d71ded956\n",
    "Acesso realizado no dia 26/09/2021"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
